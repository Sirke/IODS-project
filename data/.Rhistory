test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
#standardize the data set
boston_scaled2 <- scale(Boston)
# class of the boston_scaled object
class(boston_scaled2)
# change the object to data frame
boston_scaled2<-as.data.frame(boston_scaled2)
# euclidean distance matrix
dist_eu <- dist(boston_scaled2)
# look at the summary of the distances
summary(dist_eu)
# k-means clustering
km <-kmeans(boston_scaled2, centers = 3)
# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)
# k-means clustering
km <-kmeans(boston_scaled2, centers = 5)
# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)
# MASS, ggplot2 and Boston dataset are available
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
library(ggplot2)
# MASS, ggplot2 and Boston dataset are available
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(boston_scaled2, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)
# plot the Boston dataset with clusters
pairs(boston_scaled2[6:10], col = km$cluster)
# k-means clustering
km <-kmeans(boston_scaled2, centers = 3)
# plot the Boston dataset with clusters
pairs(boston_scaled2[6:10], col = km$cluster)
# plot the Boston dataset with clusters
pairs(boston_scaled2[9:14], col = km$cluster)
# k-means clustering
km <-kmeans(boston_scaled2, centers = 5)
# plot the Boston dataset with clusters
pairs(boston_scaled2[6,10], col = km$cluster)
# k-means clustering
km <-kmeans(boston_scaled2, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scaled2[6:10], col = km$cluster)
# plot the Boston dataset with clusters
pairs(boston_scaled2[1:6], col = km$cluster)
# plot the Boston dataset with clusters
pairs(boston_scaled2[7:14], col = km$cluster)
# plot the Boston dataset with clusters
pairs(boston_scaled2[9:14], col = km$cluster)
# plot the Boston dataset with clusters
pairs(boston_scaled2[10:14], col = km$cluster)
setwd("~/GitHub/IODS-project/data")
human<-read.csv("human.csv",header = T)
View(human)
#read in data
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
# access the stringr package
library(stringr)
# look at the structure of the GNI column in 'human'
str(human$GNI)
# remove the commas from GNI and print out a numeric version of it
str_replace(human$GNI, pattern=",", replace ="")
View(human)
# remove the commas from GNI and print out a numeric version of it
human$GNI<-str_replace(human$GNI, pattern=",", replace ="")
#convert GNI into numeric
human$GNI<-(as.numeric(human$GNI))
human<-read.csv("human.csv",header = T)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
# access the stringr package
library(stringr)
# look at the structure of the GNI column in 'human'
str(human$GNI)
# remove the commas from GNI and print out a numeric version of it
human$GNI<-str_replace(human$GNI, pattern=",", replace ="")
#convert GNI into numeric
human$GNI<-(as.numeric(human$GNI))
#keep only columns that refer to: "Country", "Edu2.FM", "Labo.FM", "Edu.Exp",
# "Life.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F"
keep_columns<-c("Country","RAT.EDU","RAT.LAB","EYE","LEB","GNI","MMR","ABR","PARL")
human<- select(human,one_of(keep_columns))
#tidyr package
library(dplyr)
human<- select(human,one_of(keep_columns))
human[complete.cases(human), ]
human<-human[complete.cases(human), ]
tail(human)
human<-read.csv("human.csv",header = T)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
# access the stringr package
library(stringr)
# look at the structure of the GNI column in 'human'
str(human$GNI)
# remove the commas from GNI and print out a numeric version of it
human$GNI<-str_replace(human$GNI, pattern=",", replace ="")
#convert GNI into numeric
human$GNI<-(as.numeric(human$GNI))
#dpyr package
library(dplyr)
#keep only columns that refer to: "Country", "Edu2.FM", "Labo.FM", "Edu.Exp",
# "Life.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F"
keep_columns<-c("Country","RAT.EDU","RAT.LAB","EYE","LEB","GNI","MMR","ABR","PARL")
human<- select(human,one_of(keep_columns))
#keep only rows with no NAs
human<-human[complete.cases(human), ]
#remove the observations which relate to regions instead of countries
remove_countries<-c("World","Sub-Saharan Africa","Arab countries","South Asia","Latin America and the Caribbean","Europe and Central Asia","East Asia and the Pacific")
subset(human, Country!=remove_countries)
#remove the observations which relate to regions instead of countries
remove_countries<-c("World","Sub-Saharan Africa","Arab States","South Asia","Latin America and the Caribbean","Europe and Central Asia","East Asia and the Pacific")
subset(human, Country!=remove_countries)
subset(human, Country!="Arab States")
human<-subset(human, Country!="Arab States")
human<-subset(human, Country!=remove_countries)
human<-subset(human, Country!=one_of(remove_countries))
human %>% filter(Country) != %in% remove_countries)
a<-human %>% filter(Country) %in% remove_countries)
a<-human %>% filter(Country) %in% remove_countries
a<-human %>% filter(Country) %in% c("World","Sub-Saharan Africa","Arab States","South Asia","Latin America and the Caribbean","Europe and Central Asia","East Asia and the Pacific")
a <- human[ !grepl(paste(remove_countries, collapse="|"), df$Country),]
human<-filter(human, Country==remove_countries)
human<-read.csv("human.csv",header = T)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
# access the stringr package
library(stringr)
# look at the structure of the GNI column in 'human'
str(human$GNI)
# remove the commas from GNI and print out a numeric version of it
human$GNI<-str_replace(human$GNI, pattern=",", replace ="")
#convert GNI into numeric
human$GNI<-(as.numeric(human$GNI))
#dpyr package
library(dplyr)
#keep only columns that refer to: "Country", "Edu2.FM", "Labo.FM", "Edu.Exp",
# "Life.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F"
keep_columns<-c("Country","RAT.EDU","RAT.LAB","EYE","LEB","GNI","MMR","ABR","PARL")
human<- select(human,one_of(keep_columns))
#keep only rows with no NAs
human<-human[complete.cases(human), ]
View(human)
a<-human[human$Country %in% c("World","Sub-Saharan Africa","Arab States","South Asia","Latin America and the Caribbean","Europe and Central Asia","East Asia and the Pacific"), ]
View(a)
b<-human[human$Country %in% remove_countries, ]
c<-human[human$Country !%in% remove_countries, ]
c<-human[human$Country != %in% remove_countries, ]
c<-human[human$Country %in% !remove_countries, ]
c<-human[human$Country != remove_countries, ]
View(c)
filter(human, Country == "World")
human<-filter(human, Country != "World")
human<-filter(human, Country != "Arab States")
human<-filter(human, Country != "Sub-Saharan Africa")
human<-filter(human, Country != "South Asia")
human<-filter(human, Country != "Latin America and the Caribbean")
human<-filter(human, Country != "Europe and Central Asia")
human<-filter(human, Country != "East Asia and the Pacific")
# add countries as rownames
rownames(human) <- human$Country
# remove the Country variable
human <- select(human, -Country)
#save
write.csv(human,"human.csv",row.names = T)
human<-read.csv("human.csv",header = T,row.names = T)
human<-read.csv("human.csv",header = T, row.names = TRUE)
#read in data
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
#explore data
str(gii)
str(hd)
dim(gii)
dim(hd)
summary(gii)
summary(hd)
#change various column names
colnames(gii)[1] <- "GII.r"
colnames(gii)[3] <- "GII"
colnames(gii)[4] <- "MMR"
colnames(gii)[5] <- "ABR"
colnames(gii)[6] <- "PARL"
colnames(gii)[7] <- "FEDU"
colnames(gii)[8] <- "MEDU"
colnames(gii)[9] <- "FLAB"
colnames(gii)[10] <- "MLAB"
colnames(hd)[1] <- "HDI.r"
colnames(hd)[3] <- "HDI"
colnames(hd)[4] <- "LEB"
colnames(hd)[5] <- "EYE"
colnames(hd)[6] <- "MYE"
colnames(hd)[7] <- "GNI"
colnames(hd)[8] <- "GNI.HDI"
#count two new variables
gii$RAT.EDU<-gii$FEDU/gii$MEDU
gii$RAT.LAB<-gii$FLAB/gii$MLAB
#combine the two datasets with inner join by variable country
human<-merge(gii, hd, by = "Country")
#check the dimensions
dim(human)
#save as csv-file
write.csv(human, "human.csv",row.names = F)
human<-read.csv("human.csv",header = T)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
# access the stringr package
library(stringr)
# look at the structure of the GNI column in 'human'
str(human$GNI)
# remove the commas from GNI and print out a numeric version of it
human$GNI<-str_replace(human$GNI, pattern=",", replace ="")
#convert GNI into numeric
human$GNI<-(as.numeric(human$GNI))
#dpyr package
library(dplyr)
#keep only columns that refer to: "Country", "Edu2.FM", "Labo.FM", "Edu.Exp",
# "Life.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F"
keep_columns<-c("Country","RAT.EDU","RAT.LAB","EYE","LEB","GNI","MMR","ABR","PARL")
human<- select(human,one_of(keep_columns))
#keep only rows with no NAs
human<-human[complete.cases(human), ]
#remove the observations which relate to regions instead of countries
library(dplyr)
human<-filter(human, Country != "World")
human<-filter(human, Country != "Arab States")
human<-filter(human, Country != "Sub-Saharan Africa")
human<-filter(human, Country != "South Asia")
human<-filter(human, Country != "Latin America and the Caribbean")
human<-filter(human, Country != "Europe and Central Asia")
human<-filter(human, Country != "East Asia and the Pacific")
# add countries as rownames
rownames(human) <- human$Country
# add countries as rownames
rownames(human)
# remove the Country variable
human <- select(human, -Country)
#save
write.csv(human,"human2.csv",row.names = T)
human<-read.csv("human2.csv",header = T, row.names = TRUE)
human<-read.csv("human2.csv",header = T, row.names= 1)
View(human)
human<-read.csv("human2.csv",header = T, row.names=1)
str(human)
summary(human)
# Access GGally
library(GGally)
# visualize the 'human_' variables
ggpairs(human_)
# visualize the 'human_' variables
ggpairs(human)
# compute the correlation matrix and visualize it with corrplot
cor(human)%>%corrplot()
library(dplyr)
# compute the correlation matrix and visualize it with corrplot
cor(human)%>%corrplot()
library(corrplot)
# compute the correlation matrix and visualize it with corrplot
cor(human)%>%corrplot()
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)
# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex=c(0.8,1),col = c("grey40", "deeppink2"))
human_std <- scale(human)
# print out summaries of the standardized variables
summary(human_std)
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)
# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex=c(0.8,1),col = c("grey40", "deeppink2"))
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(1*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex=c(0.8,1),col = c("grey40", "deeppink2"))
human_std <- scale(human)
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(1*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# print out the percentages of variance
pca_pr
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
human_std <- scale(human)
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
install.packages("FactoMineR")
library(FactoMineR)
data(tea)
View(tea)
# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))
# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") +geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
library(tidyr)
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") +geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)
# summary of the model
summary(mca)
# visualize MCA
plot(mca, invisible=c("ind"),habillage = "quali")
summary(human)
human<-read.csv("human2.csv",header = T, row.names=1)
str(human)
plot(GNI)
plot(human$GNI)
boxplot(human$GNI)
hist(human$GNI)
# Access GGally and corrplot
library(GGally)
library(corrplot)
# visualize the 'human_' variables
ggpairs(human)
# compute the correlation matrix and visualize it with corrplot
cor(human)%>%corrplot()
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
human_std <- scale(human)
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
summary(human)
# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") +geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# summary of the model
summary(mca)
# visualize MCA
plot(mca, invisible=c("ind"),habillage = "quali")
human<-read.csv("human2.csv",header = T, row.names=1)
human<-read.csv("human2.csv",header = T, row.names=1)
setwd("~/GitHub/IODS-project/data")
human<-read.csv("human2.csv",header = T, row.names=1)
human<-read.csv("C:/Users/Sirke Piirainen/Documents/GitHub/IODS-project/data/human2.csv",header = T, row.names=1)
str(human)
human<-read.csv("C:/Users/Sirke Piirainen/Documents/GitHub/IODS-project/data/human2.csv",header = T, row.names=1)
str(human)
summary(human)
hist(human$GNI)
# Access GGally and corrplot
library(GGally)
library(corrplot)
# visualize the 'human_' variables
ggpairs(human)
# compute the correlation matrix and visualize it with corrplot
cor(human)%>%corrplot()
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
human_std <- scale(human)
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
library(tidyr)
library(FactoMineR)
data(tea)
# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))
# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") +geom_bar()+theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)
# summary of the model
summary(mca)
# visualize MCA
plot(mca, invisible=c("ind"),habillage = "quali")
library(magrittr)
library(dplyr)
library(tidyr)
library(FactoMineR)
