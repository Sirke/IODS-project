#Logistic regression


##What I learned this week

This week I learned about logistic regression..

##Data exploration and selection
Let's start by reading in the data and exploring the structure and dimensions of the data:

```{r}
#read in the csv
alc<-read.csv("C:/Users/Sirke Piirainen/Documents/GitHub/IODS-project/data/alc.csv",header = T)

#print the names of the columns
names(alc)
```

This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades (G1, G2 and G3), demographic, social and school related features and it was collected by using school reports and questionnaires. Two datasets were provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). These datasets have been combined so that numerical variables are averaged and categorical values are taken directly from the Mathematics dataset. Alcohol consumption of students during week days and weekends has been combined as an average and categorized yes or no for high use. Check the variable details [here](https://archive.ics.uci.edu/ml/datasets/Student+Performance).

The data consists of 35 variables and 382 observations. 


Next I want to study the relationships between high/low alcohol consumption and some of the other variables in the data. To do this, I choose 4 interesting variables in the data and for each of them, I present a hypothesis about their relationships with alcohol consumption.

1. famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) 
I assume students with very bad realationships have high alcohol consumption.

2. absences - number of school absences (numeric: from 0 to 93)
I assume student with lots of absences have high alcohol consumption.

3. studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) 
I assume that students with low weekly study time have high alcohol consumption.

4. higher - wants to take higher education (binary: yes or no)
I assume students who don't want to take higher education have high alcohol consumption.

```{r}
#get some packages
library(tidyr); library(dplyr); library(ggplot2)

#select only the data that I am interested in
choose<-c("high_use","famrel","absences","studytime","higher")
alc2<-select(alc,one_of(choose))

#summary table of the data
summary(alc2)

#draw barplots of variables to study their distribution
gather(alc2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free")+geom_bar()

library(GGally)
library(ggplot2)

# create a plot matrix with ggpairs()
p <- ggpairs(alc2, mapping = aes(col=high_use,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
p

```
None of the variables seem to be evenly distributed. 

-There are more students that don't have a high use of alcohol than those that do.
-There are more students that want to take higher education than those who don't want to. 
-There are many students with very little absences, some with a few absences and then again many student with more absences.
-Family relations seem to be fairly good for most of the students.
-Most of the students study 2-5 hours weekly. Only few students study a lot.
```{r}
library(GGally)
library(ggplot2)

# create a plot matrix with ggpairs()
p <- ggpairs(alc2, mapping = aes(col=high_use,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
p

```
Looks like all the hypothesis that I made hold some truth although I guess I might not have enough data from all observed categories to verify my assumptions. 

##Fitting a logistic regression model
```{r}
#fit a logistic regression model
m <- glm(high_use ~ famrel + absences+studytime+higher-1, data = alc2, family = "binomial")

#plot a summary table
summary(m)
```
From the summary table I can see that the relationship:

-between high use and famrel is not significant but students with better (higher) relations are less likely in the high use category.

-between absences and high use is highly significant so that students with many absences are more likely in the high use category.

-between study time and high use is also highly significant so that student who spend more time studying are less likely in the high use category.

-between higher education and high use is not significant but is such that students with no willingness to take higher education are more likely in the high use category.


##Model selection
I study the factorial variable 'higher' a bit more. I check whether the whole variable 'higher' improves the model fit, I fit one model with (my.mod1) and one without the variable 'higher' (my.mod2) and conduct a likelihood ratio test. This tests the hypothesis that all coefficients of higher are zero:

```{r}
my.mod1 <- glm(high_use ~ famrel + absences+studytime+higher-1, data = alc2, family = "binomial") 
my.mod2 <- glm(high_use ~ famrel + absences+studytime-1, data = alc2, family = "binomial")

anova(my.mod1, my.mod2, test="LRT")
```

This test tells me that having the variable 'higher' in the model doesn't improve it so I can drop it. I fit a new model without it:


```{r}
#the new model again
m2 <- glm(high_use ~ famrel + absences+studytime, data = alc2, family = "binomial")
#summary table of the new model
summary(m2)
```
Then I could also try to drop the variable 'famrel' because it doesn't seem to be significant (p-value less than 0.05). I fit a model without 'famrel'.

```{r}
m3<-glm(formula = high_use ~ absences + studytime, family = "binomial", 
    data = alc2)

summary(m3)
```
The model without 'famrel' has a higher AIC value which is not preferable but the difference to the model including it is less than 2 so therefore I can choose the simpler model. So my final model has only absences and studytime as explaining variables.

Next I count the coefficients of the final model as odds ratios and provide confidence intervals for them:

###counting the odds

```{r}
#count odd ratios 
OR <- coef(m3) %>% exp

# compute confidence intervals (CI)
CI<-confint(m3)%>%exp

# print out the odds ratios with their confidence intervals
cbind(OR, CI)

```

Odd ratios higher than 1 means that this variable is positively associated with "success", in this case being in the high use category. In this case, the variable 'absences' is positively associated with high use and 'studytime' is negatively associated. 

From the confidence intervals I can check that 1 doesn't occur within them because it would mean that the variable has no effect on the success. I can also observe whether the variable has a lot of variation in the intervals. A large variation in the intervals indicates a low level of precision of the odds ratio, whereas a small variation in intervals indicates a higher precision of the odds ratio.

In my case studytime has rather wide confidence intervals and therefore it might have a lower precision of the odds ratio.

##Predictions

Provide a 2x2 cross tabulation of predictions versus the actual values and optionally display a graphic visualizing both the actual values and the predictions. Compute the total proportion of inaccurately classified individuals (= the training error) and comment on all the results. Compare the performance of the model with performance achieved by some simple guessing strategy.

To validate my model I use it to make predictions and compare them with the true observed values.

```{r}
# predict() the probability of high_use
probabilities <- predict(m3, type = "response")

# add the predicted probabilities to 'alc'
alc2 <- mutate(alc2, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc2 <- mutate(alc2, prediction = probability>0.5)

# see the last ten original classes, predicted probabilities, and class predictions
select(alc2, studytime, absences, high_use, probability, prediction) %>% tail(10)

# tabulate the target variable versus the predictions
table(high_use = alc2$high_use, prediction = alc2$prediction)

# tabulate the target variable versus the predictions
table(high_use = alc2$high_use, prediction = alc2$prediction)%>%prop.table()%>%addmargins()
```
Above I produced two confusion tables (in absolute numbers and in percentage) from which I can see how many or what proportion of the predictions were correct. 

Then the same thing as a plot:

```{r}
g <- ggplot(alc2, aes(x = probability, y = high_use,col=prediction))

# define the geom as points and draw the plot
g+geom_point()


```
In this plot I can see both the actual values and the predictions. I have quite a lot of predictions of falses even though they were actually true. But is not as bad as predicting true even though they were falses in reality. falc

To quantify the goodness of my model I can count the proportion of incorrect predictions. 

```{r}
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions
loss_func(class = alc2$high_use, prob = alc2$probability)


```

The result is 28 %. I could've counted that also from the cross table (25 + 3 = 28 %). On my opinion that sounds like a lot but I guess it is still better than a simple guessing strategy. 72 % for correct predictions sounds better :)

The problem with this kind of model validation is that the model is asked to predict the same values that are used to create the model. This seems like a circular argument. 

##Cross-validation
Bonus: Perform 10-fold cross-validation on your model. Does your model have better test set performance (smaller prediction error using 10-fold cross-validation) compared to the model introduced in DataCamp (which had about 0.26 error). Could you find such a model? (0-2 points to compensate any loss of points from the above exercises)

Another way of doing model validation is cross-validation. Here a proportion of the data is set aside as testing data and the model is fitted with training data only. The resulting model is then fed with testing data to make predictions. The goodness of the model is then quantified as the proportion of correct predictions. This procedure is repeated several times so that the whole dataset has been used as testing data. If the cross-validation is 5-fold it means that the data is split into 5 proportions of which 1/5 is used as testing data one at a time. Which data to select as testing data can be random or for example spatially defined.

In R cross-validation can be done easily several times so the following r code I repeat a couple of times:

```{r}
# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc2, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```
All the results that I get are close to 29-30 %. This means that with cross-validation my model seems to be performing worse than with the loss function I performed earlier (and the model used in DataCamp). I assume this is because the cross-validation procedure is more profound and therefore gives a more realistic valuation of the prediction accuracy. Without a proper model validation the results might be too over-optimistic.

##Comparing complicated and simple models with cross-validation metrics

Super-Bonus: Perform cross-validation to compare the performance of different logistic regression models (= different sets of predictors). Start with a very high number of predictors and explore the changes in the training and testing errors as you move to models with less predictors. Draw a graph displaying the trends of both training and testing errors by the number of predictors in the model. (0-4 points to compensate any loss of points from the above exercises)

m <- glm(high_use ~ famrel + absences+studytime+higher-1, data = alc2, family = "binomial")

```{r}
#the most complicated model
M1<-glm(high_use ~ famrel + absences+studytime+higher, data = alc2, family = "binomial")
M2<-glm(high_use ~        + absences+studytime+higher, data = alc2, family = "binomial")
M3<-glm(high_use ~ famrel +         +studytime+higher, data = alc2, family = "binomial")
M4<-glm(high_use ~ famrel + absences+         +higher, data = alc2, family = "binomial")
M5<-glm(high_use ~ famrel + absences+studytime+      , data = alc2, family = "binomial")
M6<-glm(high_use ~ studytime+higher, data = alc2, family = "binomial")
M7<-glm(high_use ~ famrel + higher, data = alc2, family = "binomial")
M8<-glm(high_use ~ famrel + absences, data = alc2, family = "binomial")
```




