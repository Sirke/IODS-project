#Analysis of longitudinal data

This week I learned how to handle data where the same individuals have been measured repeatedly over a time frame.

##Graphical displays and summary measure approach

###Visual exploration

First I learned how to look for differences between groups by visually exploring the data.

```{r}
#read in data
BPRSL<-read.csv("C:/Users/Sirke Piirainen/Documents/GitHub/IODS-project/data/men.csv",header = T)

#structure and summary of data
str(BPRSL)
summary(BPRSL)
```

In this BPRS data set 40 male subjects were randomly assigned to one of two treatment groups 
and each subject was rated on the brief psychiatric rating scale (BPRS) measured before 
treatment began (week 0) and then at weekly intervals for eight weeks. The BPRS assesses 
the level of 18 symptom constructs such as hostility, suspiciousness, hallucinations and 
grandiosity; each of these is rated from one (not present) to seven (extremely severe). 
The scale is used to evaluate patients suspected of having schizophrenia.

I notice that R reads integers as numerical values even though when I saved the file they were factors. I convert treatment and subject again to factors.

```{r}
# Factor treatment & subject in BPRS
BPRSL$treatment <- factor(BPRSL$treatment)
BPRSL$subject <- factor(BPRSL$subject)
```

Graphical displays of data are almost always useful for exposing patterns in the data. To begin we shall plot the BPRS values for all 40 men, differentiating between the treatment groups into which the men have been randomized. This simple graph makes a number of features of the data readily apparent.

```{r}
#Access the package ggplot2
library(ggplot2)

# Draw the plot
ggplot(BPRSL, aes(x = week, y = bprs, linetype = subject)) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = c(min(BPRSL$bprs), max(BPRSL$bprs)))
```

From these plots I can see that in both treatments bprs values are generally decreasing as time goes by. There are some individuals who have high values at the beginning of the testing and at the end of the testing, whereas for some individuals the values are notably lower all the time. 

Standardizing the bprs values makes it easier to compare the changes happening between individuals. 

Notice that here I use the function 'scale' for standardization whereas in the datacamp exercise the scaling was done manually. 

```{r}
library(magrittr)
library(dplyr)
library(tidyr)

# Standardise the variable bprs
BPRSL <- BPRSL %>%
  group_by(week) %>%
  mutate(stdbprs = scale(bprs)) %>%
  ungroup()

#check the results
glimpse(BPRSL)

# Plot again with the standardised bprs
ggplot(BPRSL, aes(x = week, y = stdbprs, linetype = subject)) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_y_continuous(name = "standardized bprs")

```

Standardizing the values made it easier to compare the differences between the individuals but in order to compare the differences between these two goups, the treatments I would like to summarize the results of the groups some how and compare them. However, I don't want to loose information on the variability of the data which was clearly visible in these plots above. 

To compare the differences between the two treatments I summarize the bprs values of the two groups by counting their weekly mean values and standard errors, and plot these values by groups. 

```{r}
# Number of weeks, baseline (week 0) included
n <- BPRSL$week %>% unique() %>% length()

# Summary data with mean and standard error of bprs by treatment and week 
BPRSS <- BPRSL %>%
  group_by(treatment, week) %>%
  summarise( mean = mean(bprs), se = sd(bprs)/sqrt(n) ) %>%
  ungroup()

# Glimpse the data
glimpse(BPRSS)

# Plot the mean profiles
ggplot(BPRSS, aes(x = week, y = mean, linetype = treatment, shape = treatment)) +
  geom_line() +
  scale_linetype_manual(values = c(1,2)) +
  geom_point(size=3) +
  scale_shape_manual(values = c(1,2)) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se, linetype="1"), width=0.3) +
  theme(legend.position = c(0.8,0.8)) +
  scale_y_continuous(name = "mean(bprs) +/- se(bprs)")
```

The mean values of the two groups during each week seem to differ somewhat but when looking at the variability (the standard error bars) in both groups it is obvious that the values of the two groups are overlapping a lot. This suggests that the differences between the two groups might not be significant.

Summarizing the information in the dataset even more by taking the mean values of bprs of all the weeks let's us compare the treatments as a whole. This is an example of using a summary measure approach. The mean of weeks 1 to 8 will be my summary measure.

```{r}
# Create a summary data by treatment and subject with mean as the summary variable (ignoring baseline week 0).
BPRSL8S <- BPRSL %>%
  filter(week > 0) %>%
  group_by(treatment, subject) %>%
  summarise( mean=mean(bprs) ) %>%
  ungroup()

# Glimpse the data
glimpse(BPRSL8S)

# Draw a boxplot of the mean versus treatment
ggplot(BPRSL8S, aes(x = treatment, y = mean)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape=23, size=4, fill = "white") +
  scale_y_continuous(name = "mean(bprs), weeks 1-8")
```

The mean summary measure is more variable in the second treatment group and its distribution in this group is somewhat skew. The boxplot of the second group also reveals an outlier, a subject whose mean BPRS score of the eight weeks is over 70. It might bias the conclusions from further comparisons of the groups, so I remove that subject from the data. 

```{r}
# Create a new data by filtering the outlier and adjust the ggplot code the draw the plot again with the new data
BPRSL8S1 <- BPRSL8S %>%
  filter(mean < 70) 
  
glimpse(BPRSL8S1)

ggplot(BPRSL8S1, aes(x = treatment, y = mean)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape=23, size=4, fill = "white") +
  scale_y_continuous(name = "mean(bprs), weeks 1-8")
```

Removing the outlier lowered considerably the mean value of the treatment 2 group. I wouldn't remove outliers unless there was a clear reason for doing so. If this value (BPRS>70) was clearly an error (typing error on the notes) or it came from a person who was somehow uncomparable with the other participants (has suffered somekind of trauma or has an unnormal medical condition).


###Statistical analysis

Even though the visual exploration of the differences between the two treatments clearly suggests that there are no differences between the groups it is adviceable to test this statistically. A simple test is the t-test that compares the mean values of the two groups.

```{r}
# Perform a two-sample t-test
t.test(mean ~ treatment, data = BPRSL8S1, var.equal = TRUE)
```

The t-test confirms the lack of any evidence for a group difference. Also the 95% confidence interval is wide and includes the zero, allowing for similar conclusions to be made.

The data includes information on the bprs values of each individual before any treatment was applied. This value can be called the baseline. For some individuals it was higher than for others and it also stays high throughout the treatment period. This value is correlated with the mean BPRS values and therefore it might be a good idea to take it into account when testing for differences between the groups. Adding information about the baseline in the analysis can often lead to substantial gains in precision when used appropriately as a covariate in an analysis of covariance.

I fit a linear model with mean BPRS as the response variable and treatment and baseline as explaining factors.

```{r}
BPRS <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/MABS/master/Examples/data/BPRS.txt", sep  =" ", header = T)

# Add the baseline from the original data as a new variable to the summary data
BPRSL8S2 <- BPRSL8S %>%
  mutate(baseline = BPRS$week0)

# Fit the linear model with the mean as the response 
fit <- lm(mean~baseline+treatment, data = BPRSL8S2)

# Compute the analysis of variance table for the fitted model with anova()
summary(fit)
```

From the summary table I can see that the baseline BPRS is strongly related (p-value < 0.001) to the BPRS values taken after treatment has begun, but there is still no evidence of a treatment difference (p-value=0.81) even after conditioning on the baseline value.


##Linear mixed effects models

###Exploring the rat data visually

Longitudinal data, where a response variable is measured on each subject on several different occasions poses problems for their analysis because the repeated measurements on each subject are very likely to be correlated rather than independent. Next I learned about methods for dealing with longitudinal data which aim to account for the correlated nature of the data and where the response is assumed to be normally distributed.

To investigate the use of linear mixed effects models I use data from a nutrition study conducted in three groups of rats. The groups were put on different diets, and each animal's body weight (grams) was recorded repeatedly (approximately) weekly, except in week seven when two recordings were taken) over a 9-week period. 

```{r}
#read in data
RATSL<-read.csv("C:/Users/Sirke Piirainen/Documents/GitHub/IODS-project/data/rats.csv",header = T)

# Factor ID and Group
RATSL$ID <- factor(RATSL$ID)
RATSL$Group<- factor(RATSL$Group)

#structure and summary of data
str(RATSL)
summary(RATSL)
```

Plotting the values makes it easier to see differences between individuals and groups.

```{r}
# Plot the RATSL data
ggplot(RATSL, aes(x = Time, y = Weight, group = ID)) +
  geom_line(aes(linetype=Group))+scale_x_continuous(name = "Time (days)", breaks = seq(0, 60, 10))+scale_y_continuous(name = "Weight (grams)")+theme(legend.position = "top")
```

From the plot I can see that there are substantial differences between the groups of rats. Already at the beginning of the test the rats represented very different weight classes. In all the groups the rats gained weight as time went by. Maybe more so in groups 2 and 3.

###Basic linear model applied on nested data

If we were to ignore the fact that the measurements came from the same individuals we could interpret the values as independent and use simple linear model to analyse the data. This is wrong but I do it for didactical reasons.

I fit a linear model with weight as the response variable and time and group as explaining variables:

```{r}
# create a regression model RATS_reg
RATS_reg <- lm(Weight~Time+Group,data=RATSL)

# print out a summary of the model
summary(RATS_reg)
```

From the model's summary table I can see that both explanatory variables are statistically significant and that group 1 differs substantially from groups 2 and 3.

###Random intercept model

I don't want to ignore the fact that measurements were taken from the same rats. I fit a random intercept model that allows the linear regression fit for each rat to differ in intercept from other rats. I include the variable ID as a random component in order to distinguish between individuals. 

```{r}
# access library lme4
library(lme4)

# Create a random intercept model
RATS_ref <- lmer(Weight ~ Time + Group + (1 | ID), data = RATSL, REML = FALSE)

# Print the summary of the model
summary(RATS_ref)
```

From the summary output I can interpret that there is a significant difference in the growth rates of rats in group 1 compared to groups 2 and 3. 

###Random intercept and slope model

Next I fit a random intercept and random slope model that allows the linear regression fits for each individual to differ in intercept but also in slope. This way it is possible to account for the individual differences in the rats' growth profiles, but also the effect of time.

```{r}
# create a random intercept and random slope model
RATS_ref1 <- lmer(Weight ~ Time + Group + (Time | ID), data = RATSL, REML = FALSE)

# print a summary of the model
summary(RATS_ref1)
```

Which one of the models is better? I perform an ANOVA test on the two models:

```{r}
# perform an ANOVA test on the two models
anova(RATS_ref1, RATS_ref)
```

There is a significant difference between the two models and seems like RATS_ref1 i.e. the random intercept and slope model has a better fit. I am personally accustomed to looking at the AIC values and comparing them.

###Adding interaction

Next I want to try a model that also includes an interaction term between time and group and compare this model with the previous model without the interaction:

```{r}
# create a random intercept and random slope model with the interaction
RATS_ref2 <- lmer(Weight ~ Time + Group + Time*Group + (Time | ID), data = RATSL, REML = FALSE)

# print a summary of the model
summary(RATS_ref2)
```

Comparison with an anova:

```{r}
# perform an ANOVA test on the two models
anova(RATS_ref2, RATS_ref1)
```

It looks like the model including the interaction term has a better fit.

###Visual expection of the model fit

Next I draw two plots. One showing the original observations and one showing fitted values from the model.

```{r}
# draw the plot of RATSL with the observed Weight values
ggplot(RATSL, aes(x = Time, y = Weight, group = ID)) +
  geom_line(aes(linetype = Group)) +
  scale_x_continuous(name = "Time (days)", breaks = seq(0, 60, 20)) +
  scale_y_continuous(name = "Observed weight (grams)") +
  theme(legend.position = "top")

# Create a vector of the fitted values
Fitted <- fitted(RATS_ref2)

# Create a new column fitted to RATSL
RATSL<-mutate(RATSL,Fitted)

# draw the plot of RATSL with the Fitted values of weight
ggplot(RATSL, aes(x = Time, y = Fitted, group = ID)) +
  geom_line(aes(linetype = Group)) +
  scale_x_continuous(name = "Time (days)", breaks = seq(0, 60, 20)) +
  scale_y_continuous(name = "Fitted weight (grams)") +
  theme(legend.position = "top")
```

Visual inspection of the model shows that the model I created (with random intercept and slope as well as the interaction term) seems to fit very well. The growth rates of rats is explained by the group the rat belongs to, by the time that has passed and the combination of these two. 

